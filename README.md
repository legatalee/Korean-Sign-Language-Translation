# LSTM 기반 한국 수어와 한국어 간 양방향 실시간 번역 프로젝트

## 프로젝트 개요

본 프로젝트는 인공지능 영상 인식을 활용하여 한국수어를 한국어로 변환하는 기능과, 한국어 음성을 3D 모델을 통해 수어로 변환하는 기능을 구현하여 양방향 실시간 번역이 가능하도록 합니다.

[![한국어와 한국 수어 간 양방향 인공지능 번역 시스템](https://img.youtube.com/vi/flWaCQA7Nno/0.jpg)](https://www.youtube.com/watch?v=flWaCQA7Nno)


## 사용 기술

- **영상 처리 및 특징 추출**: OpenCV, MediaPipe
- **머신러닝**: TensorFlow/Keras
- **지문자 인식**: XGBoost
- **3D 모델 및 애니메이션**: Unity

## 프로젝트 상세

### 1. 일반 수어 번역

1. 각 수어 단어별 30프레임씩 20개의 영상을 촬영합니다.
2. MediaPipe를 사용하여 카메라에 인식된 사람의 양 손가락 특징점과, 양 팔의 손목, 팔꿈치, 어깨 총 6개의 3차원 좌표를 추출하여 numpy 배열로 변환합니다.
3. 95%의 데이터를 Train Set으로, 5%의 데이터를 Test Set으로 임의로 분리합니다.
4. 4개의 레이어로 구성된 학습 모델에 데이터를 학습시킵니다.
5. 1000 Epochs 동안 모델을 학습(fitting)합니다.

### 2. 학습 모델 레이어 구성

- **Layer 1**: LSTM, Neuron: 64, 활성화 함수: ReLU
- **Layer 2**: LSTM, Neuron: 64, 활성화 함수: ReLU
- **DropOut 50%**: 과적합 방지
- **Layer 3**: Dense(Fully Connected), Neuron: 64, 활성화 함수: ReLU
- **Layer 4**: Dense(Fully Connected), Neuron: 64, 활성화 함수: Softmax

수어는 일련의 시퀀스를 통해 판별해야 하므로 LSTM을 학습 레이어에 적용하였습니다. 또한, 두 번째와 세 번째 레이어 사이에 DropOut을 적용하여 모델의 과적합을 방지했습니다. 활성화 함수는 ReLU를 사용하였으며, 마지막 출력 레이어에는 Softmax를 사용하였습니다.

### 3. 지문자 인식

- MediaPipe로 인식한 손가락 특징점을 XGBoost를 사용하여 분류하고 학습합니다.
- 학습 모델을 통해 지문자를 한글 단모 단자로 변환합니다.
- 중복 입력 필터 처리를 통해 지문자 입력 시퀀스를 배열로 저장합니다.
- API 호출을 통해 한글 자모음 시퀀스를 단어로 조합합니다.
- 단모와 단자가 연속으로 나오는 경우 필터링을 하도록 구현했습니다.

#### 예시

입력: `[ㅇ, ㅕ, ㅇ, ㅁ, ㅅ, ㅏ, ㅇ]` → 변환 과정: `영ㅁ상` → 최종 출력: `영상`

이후 API 응답으로 받은 단어를 출력 배열에 추가합니다.

## 라이선스

본 프로젝트는 MIT 라이선스를 따릅니다.

